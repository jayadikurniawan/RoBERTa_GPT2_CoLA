{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80a9ea0d-0b56-429d-8009-2008e5862af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/basilmusyaffa19/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import time\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset, WeightedRandomSampler\n",
    "from transformers import RobertaTokenizer, RobertaModel, get_linear_schedule_with_warmup\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report, confusion_matrix, matthews_corrcoef\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87be82a9-9ee2-4214-8766-2cb0b66669ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"nyu-mll/glue\", \"cola\")\n",
    "\n",
    "training_data = dataset[\"train\"]\n",
    "validation_data = dataset[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad55cd78-ea0f-4180-a09e-7d3d6e3f7d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(training_data)\n",
    "df_test = pd.DataFrame(validation_data) # Sebagai data test, karena memiliki label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b57829f-06a1-421b-8b1a-5054f40ea23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop('idx', axis=1)\n",
    "df_test = df_test.drop('idx', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928b8f1d-7380-4c71-8d1d-31a7ca5a0de4",
   "metadata": {},
   "source": [
    "Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cadb065-3fc6-4527-afa6-fe56e7447b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_text(text):\n",
    "    text = text.lower()\n",
    "    text = ' '.join(text.split())\n",
    "    text = text.encode('ascii', 'ignore').decode()\n",
    "    text = re.sub(r'([.,!?])', r'\\1 ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e449b802-5633-46af-aa31-597a002907c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['sentence']=df_train['sentence'].apply(cleaning_text)\n",
    "df_test['sentence']=df_test['sentence'].apply(cleaning_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "078fb61f-a3ed-4517-870d-7903a7d7fa64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>our friends won't buy this analysis, let alone...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>one more pseudo generalization and i'm giving ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>one more pseudo generalization or i'm giving up.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the more we study verbs, the crazier they get.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>day by day the facts are getting murkier.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8546</th>\n",
       "      <td>poseidon appears to own a dragon</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8547</th>\n",
       "      <td>digitize is my happiest memory</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8548</th>\n",
       "      <td>it is easy to slay the gorgon.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8549</th>\n",
       "      <td>i had the strangest feeling that i knew you.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8550</th>\n",
       "      <td>what all did you get for christmas?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8551 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence  label\n",
       "0     our friends won't buy this analysis, let alone...      1\n",
       "1     one more pseudo generalization and i'm giving ...      1\n",
       "2     one more pseudo generalization or i'm giving up.       1\n",
       "3       the more we study verbs, the crazier they get.       1\n",
       "4            day by day the facts are getting murkier.       1\n",
       "...                                                 ...    ...\n",
       "8546                   poseidon appears to own a dragon      0\n",
       "8547                     digitize is my happiest memory      0\n",
       "8548                    it is easy to slay the gorgon.       1\n",
       "8549      i had the strangest feeling that i knew you.       1\n",
       "8550               what all did you get for christmas?       1\n",
       "\n",
       "[8551 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99765264-7c5c-4893-8793-39628503dee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah data sebelum: 8551\n",
      "Jumlah data setelah: 8530\n"
     ]
    }
   ],
   "source": [
    "print(\"Jumlah data sebelum:\", len(df_train))\n",
    "\n",
    "# 1. Menghapus data kosong dan NaN sekaligus\n",
    "df_train = df_train.replace('', np.nan).dropna(subset=['sentence'])\n",
    "# 2. Menghapus data yang memiliki tipe float -> Teksnya hanya angka\n",
    "df_train = df_train[~df_train['sentence'].apply(lambda x: isinstance(x, float))]\n",
    "# 3. Menghapus duplikat, dengan mempertahankan data pertama\n",
    "df_train = df_train.drop_duplicates(subset=['sentence'], keep='first')\n",
    "\n",
    "print(\"Jumlah data setelah:\", len(df_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff34a27c-b92d-48ad-b2e6-48305a7b4588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah data sebelum: 1043\n",
      "Jumlah data setelah: 1039\n"
     ]
    }
   ],
   "source": [
    "print(\"Jumlah data sebelum:\", len(df_test))\n",
    "\n",
    "# 1. Menghapus data kosong dan NaN sekaligus\n",
    "df_test = df_test.replace('', np.nan).dropna(subset=['sentence'])\n",
    "# 2. Menghapus data yang memiliki tipe float -> Teksnya hanya angka\n",
    "df_test = df_test[~df_test['sentence'].apply(lambda x: isinstance(x, float))]\n",
    "# 3. Menghapus duplikat, dengan mempertahankan data pertama\n",
    "df_test = df_test.drop_duplicates(subset=['sentence'], keep='first')\n",
    "\n",
    "print(\"Jumlah data setelah:\", len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5db0139f-34f2-4d67-be57-30958df9a2e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "1    6012\n",
      "0    2518\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "label = df_train['label'].value_counts()\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "019e6c49-104b-449a-925e-312d8da00ecf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "232"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length = int(df_train['sentence'].str.len().max())\n",
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15d4b52d-bc8d-4c39-b3ca-c036a4abf588",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_loader(df, tokenizer, max_length, val_size, batch_size, random_state=42, use_weighted_sampler=True):\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    encoded_data = tokenizer(\n",
    "        df['sentence'].tolist(),\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    # Split data\n",
    "    input_ids = encoded_data['input_ids'].numpy()  \n",
    "    attention_mask = encoded_data['attention_mask'].numpy()\n",
    "    labels = df['label'].values  \n",
    "    \n",
    "    # Train test split\n",
    "    train_inputs, val_inputs, train_masks, val_masks, train_labels, val_labels = train_test_split(\n",
    "        input_ids, attention_mask, labels,\n",
    "        test_size=val_size,\n",
    "        random_state=random_state,\n",
    "        stratify=labels\n",
    "    )\n",
    "    \n",
    "    # Convert back to tensors after split\n",
    "    train_inputs = torch.tensor(train_inputs)\n",
    "    val_inputs = torch.tensor(val_inputs)\n",
    "    train_masks = torch.tensor(train_masks)\n",
    "    val_masks = torch.tensor(val_masks)\n",
    "    train_labels = torch.tensor(train_labels, dtype=torch.long)\n",
    "    val_labels = torch.tensor(val_labels, dtype=torch.long)\n",
    "    \n",
    "    # Create dataset\n",
    "    train_dataset = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "    val_dataset = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "    \n",
    "    # Create dataloader\n",
    "    if use_weighted_sampler:\n",
    "        # Hitung sample weights\n",
    "        sample_weights = compute_sample_weights(train_labels)\n",
    "        \n",
    "        # Gunakan Weighted Random Sampler\n",
    "        train_sampler = WeightedRandomSampler(\n",
    "            weights=sample_weights, \n",
    "            num_samples=len(train_dataset), \n",
    "            replacement=True\n",
    "        )\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "    else:\n",
    "        # Jika tidak menggunakan weighted sampler\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Validation loader tetap sama\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39852e1f-6273-459e-ab65-2cdb874a3081",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_loader(df, tokenizer, max_length, val_size, batch_size, random_state=42):\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    encoded_data = tokenizer(\n",
    "        df['sentence'].tolist(),\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    # Split data\n",
    "    input_ids = encoded_data['input_ids'].numpy()  \n",
    "    attention_mask = encoded_data['attention_mask'].numpy()\n",
    "    labels = df['label'].values  \n",
    "    \n",
    "    # Train test split\n",
    "    train_inputs, val_inputs, train_masks, val_masks, train_labels, val_labels = train_test_split(\n",
    "        input_ids, attention_mask, labels,\n",
    "        test_size=val_size,\n",
    "        random_state=random_state,\n",
    "        stratify=labels\n",
    "    )\n",
    "    \n",
    "    # Convert back to tensors after split\n",
    "    train_inputs = torch.tensor(train_inputs)\n",
    "    val_inputs = torch.tensor(val_inputs)\n",
    "    train_masks = torch.tensor(train_masks)\n",
    "    val_masks = torch.tensor(val_masks)\n",
    "    train_labels = torch.tensor(train_labels, dtype=torch.long)\n",
    "    val_labels = torch.tensor(val_labels, dtype=torch.long)\n",
    "    \n",
    "    # Create dataset\n",
    "    train_dataset = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "    val_dataset = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "    \n",
    "    # Hitung sample weights untuk train_labels\n",
    "    class_counts = torch.bincount(train_labels)\n",
    "    sample_weights = 1.0 / class_counts[train_labels].float()\n",
    "    sample_weights = sample_weights / sample_weights.sum()\n",
    "    \n",
    "    train_sampler = WeightedRandomSampler(\n",
    "        weights=sample_weights, \n",
    "        num_samples=len(train_dataset),\n",
    "        replacement=True\n",
    "    )\n",
    "    \n",
    "    # Create dataloader\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        sampler=train_sampler\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4fc699cd-ac2c-477d-b1a3-d4d157bcd21e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "500053dc-aeb7-4082-abd5-7e25f3515587",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/basilmusyaffa19/.conda/envs/skripsi_2/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-large')\n",
    "base_model = RobertaModel.from_pretrained('roberta-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65ae1a39-64d2-48a1-949f-2d525dda86a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self, base_model, num_labels, class_weights):\n",
    "        super(ClassificationModel, self).__init__()\n",
    "        self.base_model = base_model\n",
    "        self.dropout = nn.Dropout(0.2)  # 0.2 for RoBERTa, 0.3 for GPT2\n",
    "        self.classifier = nn.Linear(self.base_model.config.hidden_size, num_labels)\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "        self.attention_layer = nn.Sequential(\n",
    "            nn.Linear(self.base_model.config.hidden_size, self.base_model.config.hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(self.base_model.config.hidden_size, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        outputs = self.base_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        # Calculate attention scores using the complex attention layer\n",
    "        attention_scores = self.attention_layer(hidden_states)\n",
    "        \n",
    "        # Handle attention mask\n",
    "        if attention_mask is not None:\n",
    "            # Expand attention mask to match attention scores shape\n",
    "            attention_mask = attention_mask.unsqueeze(-1)\n",
    "            # Apply mask by multiplication\n",
    "            attention_scores = attention_scores * attention_mask\n",
    "            # Normalize scores after masking\n",
    "            attention_scores = attention_scores / (attention_scores.sum(dim=1, keepdim=True) + 1e-9)\n",
    "\n",
    "        # Apply attention pooling\n",
    "        pooled_output = torch.sum(hidden_states * attention_scores, dim=1)\n",
    "\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            weights = self.class_weights.to(logits.device)\n",
    "            loss_fct = nn.CrossEntropyLoss(weight=weights)\n",
    "            loss = loss_fct(logits, labels)\n",
    "\n",
    "        return loss, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40e07b86-568d-48c4-978a-91f0d2a26040",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience, min_delta, verbose=True):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "        \n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec3fe1c3-8ec1-42d4-82ac-c6d5208d4432",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(train_losses, val_losses, train_f1_scores, val_f1_scores, train_mcc_scores, val_mcc_scores):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Plot Loss\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.title('Learning Curves - Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot F1 Score\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(train_f1_scores, label='Training F1 Score')\n",
    "    plt.plot(val_f1_scores, label='Validation F1 Score')\n",
    "    plt.title('Learning Curves - F1 Score')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot MCC Score\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(train_mcc_scores, label='Training MCC Score')\n",
    "    plt.plot(val_mcc_scores, label='Validation MCC Score')\n",
    "    plt.title('Learning Curves - MCC Score')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MCC Score')\n",
    "    plt.legend()\n",
    "    \n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('learning_curves.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d4d02061-e3ea-4e08-b312-d46fef9bb7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, optimizer, device, epochs, save_path, patience, min_delta):\n",
    "    model = model.to(device)\n",
    "    best_val_loss = float('inf')\n",
    "    best_val_f1 = 0\n",
    "    best_epoch = -1\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_f1_scores = []\n",
    "    val_f1_scores = []\n",
    "    train_mcc_scores = []\n",
    "    val_mcc_scores = []\n",
    "    \n",
    "    total_training_steps = len(train_loader) * epochs\n",
    "    \n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, \n",
    "        num_warmup_steps=0, \n",
    "        num_training_steps=total_training_steps\n",
    "    )\n",
    "    \n",
    "    early_stopping = EarlyStopping(patience=patience, min_delta=min_delta)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        start_time = time.time()\n",
    "        total_train_loss = 0\n",
    "        train_preds = []\n",
    "        train_true_labels = []\n",
    "        \n",
    "        for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}/{epochs}\"):\n",
    "            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            # Perhatikan perubahan di sini\n",
    "            loss, logits = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step() \n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "            \n",
    "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            train_preds.extend(preds)\n",
    "            train_true_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_f1 = f1_score(train_true_labels, train_preds, average='weighted')\n",
    "        train_mcc = matthews_corrcoef(train_true_labels, train_preds)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        val_preds = []\n",
    "        val_true_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=f\"Validation Epoch {epoch + 1}/{epochs}\"):\n",
    "                input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "                \n",
    "                loss, logits = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels\n",
    "                )\n",
    "                \n",
    "                total_val_loss += loss.item()\n",
    "                \n",
    "                preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "                val_preds.extend(preds)\n",
    "                val_true_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        val_f1 = f1_score(val_true_labels, val_preds, average='weighted')\n",
    "        val_mcc = matthews_corrcoef(val_true_labels, val_preds)\n",
    "        \n",
    "        # Simpan history\n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        train_f1_scores.append(train_f1)\n",
    "        val_f1_scores.append(val_f1)\n",
    "        train_mcc_scores.append(train_mcc)\n",
    "        val_mcc_scores.append(val_mcc)\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        print(f\"Training Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "        print(f\"Training F1 Score: {train_f1:.4f}\")\n",
    "        print(f\"Validation F1 Score: {val_f1:.4f}\")\n",
    "        print(f\"Training MCC Score: {train_mcc:.4f}\")\n",
    "        print(f\"Validation MCC Score: {val_mcc:.4f}\")\n",
    "        print(f\"Time: {elapsed_time:.2f}s\")\n",
    "        \n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_epoch = epoch + 1\n",
    "            print(f\"Best Validation Loss: {best_val_loss:.4f}\")\n",
    "            \n",
    "            os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "            \n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(), \n",
    "            }, save_path)\n",
    "        else:\n",
    "            print(f\"Best Validation Loss {best_val_loss:.4f} (epoch {best_epoch})\")\n",
    "        \n",
    "        early_stopping(avg_val_loss)\n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"Early stopping triggered after epoch {epoch + 1}\")\n",
    "            break\n",
    "    \n",
    "    plot_learning_curves(train_losses, val_losses, train_f1_scores, val_f1_scores, train_mcc_scores, val_mcc_scores)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f66fcb4c-1b6e-4d36-b669-6009df0fd69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "            \n",
    "            loss, logits = model(input_ids, attention_mask, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            \n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    average_loss = total_loss / len(dataloader)\n",
    "    \n",
    "    accuracy = np.mean(np.array(all_predictions) == np.array(all_labels))\n",
    "    print(f\"Test Loss: {average_loss:.4f}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(all_labels, all_predictions)\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "    \n",
    "    # Classification Report\n",
    "    report = classification_report(all_labels, all_predictions)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(report)\n",
    "    \n",
    "    mcc = matthews_corrcoef(all_labels, all_predictions)\n",
    "    print(\"\\nMCC Score:\")\n",
    "    print(mcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b626a9ec-4adf-468d-a5ad-20cc4074a5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=32\n",
    "EPOCHS=7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "99f4b20d-9501-44cc-a5ef-5243408e2e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader = prepare_data_loader(\n",
    "    df_train,\n",
    "    tokenizer,\n",
    "    max_length=max_length,\n",
    "    val_size=0.1,\n",
    "    batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "95628140-c3c2-4781-90bb-27a882f2dc8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.6938, 0.7094])\n"
     ]
    }
   ],
   "source": [
    "def compute_class_weights(labels):\n",
    "    labels_tensor = torch.tensor(labels.values, dtype=torch.long) \n",
    "    \n",
    "    class_counts = torch.bincount(labels_tensor)\n",
    "    total_samples = len(labels)\n",
    "    weights = total_samples / (len(class_counts) * class_counts.float())\n",
    "    return weights\n",
    "\n",
    "class_weights = compute_class_weights(df_train['label'])\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "816add3b-0d55-4462-b48c-45c25cec90a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(\n",
    "    base_model.parameters(),\n",
    "    lr=2e-5,\n",
    "    eps=1e-8,\n",
    "    weight_decay=1e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1f808d4c-1989-497e-a697-120625a6f487",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = 2\n",
    "classification_model = ClassificationModel(base_model=base_model, \n",
    "                                           num_labels=num_labels,\n",
    "                                           class_weights=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f190fbd1-27fb-44cb-9f15-d74462694829",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/7: 100%|██████████| 240/240 [03:42<00:00,  1.08it/s]\n",
      "Validation Epoch 1/7: 100%|██████████| 27/27 [00:07<00:00,  3.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "Training Loss: 0.4276\n",
      "Validation Loss: 0.4232\n",
      "Training F1 Score: 0.7407\n",
      "Validation F1 Score: 0.8579\n",
      "Training MCC Score: 0.5235\n",
      "Validation MCC Score: 0.6615\n",
      "Time: 230.20s\n",
      "Best Validation Loss: 0.4232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/7: 100%|██████████| 240/240 [03:40<00:00,  1.09it/s]\n",
      "Validation Epoch 2/7: 100%|██████████| 27/27 [00:07<00:00,  3.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/7\n",
      "Training Loss: 0.2420\n",
      "Validation Loss: 0.4479\n",
      "Training F1 Score: 0.8894\n",
      "Validation F1 Score: 0.8615\n",
      "Training MCC Score: 0.7840\n",
      "Validation MCC Score: 0.6669\n",
      "Time: 228.82s\n",
      "Best Validation Loss 0.4232 (epoch 1)\n",
      "EarlyStopping counter: 1 out of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/7: 100%|██████████| 240/240 [03:40<00:00,  1.09it/s]\n",
      "Validation Epoch 3/7: 100%|██████████| 27/27 [00:07<00:00,  3.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/7\n",
      "Training Loss: 0.1528\n",
      "Validation Loss: 0.6118\n",
      "Training F1 Score: 0.9321\n",
      "Validation F1 Score: 0.8703\n",
      "Training MCC Score: 0.8664\n",
      "Validation MCC Score: 0.6869\n",
      "Time: 228.70s\n",
      "Best Validation Loss 0.4232 (epoch 1)\n",
      "EarlyStopping counter: 2 out of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/7: 100%|██████████| 240/240 [03:40<00:00,  1.09it/s]\n",
      "Validation Epoch 4/7: 100%|██████████| 27/27 [00:07<00:00,  3.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/7\n",
      "Training Loss: 0.0964\n",
      "Validation Loss: 0.6278\n",
      "Training F1 Score: 0.9570\n",
      "Validation F1 Score: 0.8762\n",
      "Training MCC Score: 0.9152\n",
      "Validation MCC Score: 0.7036\n",
      "Time: 228.52s\n",
      "Best Validation Loss 0.4232 (epoch 1)\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping triggered after epoch 4\n"
     ]
    }
   ],
   "source": [
    "model = train_model(model=classification_model, \n",
    "                    train_loader=train_loader,\n",
    "                    val_loader=val_loader,\n",
    "                    optimizer=optimizer, \n",
    "                    device=device, \n",
    "                    epochs=EPOCHS,\n",
    "                    save_path='/home/basilmusyaffa19/CoLA/Grammatical/model-2/With Attention Pooling/roBERTa_lowercase_best_checkpoint.pt',\n",
    "                    patience=3,\n",
    "                    min_delta=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fd82f6ff-9dd5-43fb-8cd3-f1d76af95e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_saved_model(model, save_path, device):\n",
    "    checkpoint = torch.load(save_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model = model.to(device)\n",
    "    model.eval() \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "593b49c6-17ff-4714-bea5-b92fdb0ff541",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = load_saved_model(\n",
    "    model=classification_model, \n",
    "    save_path='/home/basilmusyaffa19/CoLA/Grammatical/model-2/With Attention Pooling/roBERTa_lowercase_best_checkpoint.pt',\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b8d58c0d-fc9e-4ead-b7ac-61810b79e211",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length_test = int(df_test['sentence'].str.len().max())\n",
    "\n",
    "encoded_data_test = tokenizer(\n",
    "        df_test['sentence'].tolist(),\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "test_inputs = encoded_data_test['input_ids']  \n",
    "test_masks = encoded_data_test['attention_mask']\n",
    "test_labels = torch.tensor(df_test['label'].values, dtype=torch.long)\n",
    "    \n",
    "test_dataset = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "52f7600c-e3c4-43aa-a38f-2841cff248a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 33/33 [00:09<00:00,  3.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.4262\n",
      "Accuracy: 0.8412\n",
      "\n",
      "Confusion Matrix:\n",
      "[[244  76]\n",
      " [ 89 630]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.76      0.75       320\n",
      "           1       0.89      0.88      0.88       719\n",
      "\n",
      "    accuracy                           0.84      1039\n",
      "   macro avg       0.81      0.82      0.82      1039\n",
      "weighted avg       0.84      0.84      0.84      1039\n",
      "\n",
      "\n",
      "MCC Score:\n",
      "0.6318637210946597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(best_model, \n",
    "               test_loader, \n",
    "               device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5aff2a1-f211-410e-be7e-fb0a5bb06a90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Skripsi_2",
   "language": "python",
   "name": "skripsi_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
